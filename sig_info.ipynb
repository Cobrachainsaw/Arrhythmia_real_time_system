{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mealpy.evolutionary_based.GA import BaseGA\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from mealpy import FloatVar\n",
    "from mealpy import IntegerVar\n",
    "import time\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path('C:/Users/vinay/Downloads/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files=[]\n",
    "annot_files=[]\n",
    "for file in os.listdir(file_path):\n",
    "    if('.dat' in file):\n",
    "        data_files.append(file[:-4])\n",
    "    elif('.atr' in file):\n",
    "        annot_files.append(file[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'+': 0, 'N': 1, 'A': 2, 'V': 3, '~': 4, '|': 5, 'Q': 6, '/': 7, 'f': 8, 'x': 9, 'F': 10, 'j': 11, 'L': 12, 'a': 13, 'J': 14, 'R': 15, '[': 16, '!': 17, ']': 18, 'E': 19, 'S': 20, '\"': 21, 'e': 22}\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {}\n",
    "count = 0 \n",
    "\n",
    "for file in annot_files:\n",
    "    path_file = os.path.join(file_path, file)\n",
    "    annotation = wfdb.rdann(path_file, 'atr') \n",
    "    \n",
    "    for symbol in annotation.symbol:\n",
    "        if symbol not in char_to_int: \n",
    "            char_to_int[symbol] = count\n",
    "            count += 1 \n",
    "\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'+': 1, 'A': 33, 'N': 2239, 'V': 1} 2274 2274\n",
      "101 {'+': 1, 'A': 3, 'N': 1860, 'Q': 2, '|': 4, '~': 4} 1874 1874\n",
      "102 {'+': 5, '/': 2028, 'N': 99, 'V': 4, 'f': 56} 2192 2192\n",
      "103 {'+': 1, 'A': 2, 'N': 2082, '~': 6} 2091 2091\n",
      "104 {'+': 45, '/': 1380, 'N': 163, 'Q': 18, 'V': 2, 'f': 666, '~': 37} 2311 2311\n",
      "105 {'+': 1, 'N': 2526, 'Q': 5, 'V': 41, '|': 30, '~': 88} 2691 2691\n",
      "106 {'+': 41, 'N': 1507, 'V': 520, '~': 30} 2098 2098\n",
      "107 {'+': 1, '/': 2078, 'V': 59, '~': 2} 2140 2140\n",
      "108 {'+': 1, 'A': 4, 'F': 2, 'N': 1739, 'V': 17, 'j': 1, 'x': 11, '|': 8, '~': 41} 1824 1824\n",
      "109 {'+': 1, 'F': 2, 'L': 2492, 'V': 38, '~': 2} 2535 2535\n",
      "111 {'+': 1, 'L': 2123, 'V': 1, '~': 8} 2133 2133\n",
      "112 {'+': 1, 'A': 2, 'N': 2537, '~': 10} 2550 2550\n",
      "113 {'+': 1, 'N': 1789, 'a': 6} 1796 1796\n",
      "114 {'+': 3, 'A': 10, 'F': 4, 'J': 2, 'N': 1820, 'V': 43, '|': 1, '~': 7} 1890 1890\n",
      "115 {'+': 1, 'N': 1953, '|': 6, '~': 2} 1962 1962\n",
      "116 {'+': 1, 'A': 1, 'N': 2302, 'V': 109, '~': 8} 2421 2421\n",
      "117 {'+': 1, 'A': 1, 'N': 1534, '~': 3} 1539 1539\n",
      "118 {'+': 1, 'A': 96, 'R': 2166, 'V': 16, 'x': 10, '~': 12} 2301 2301\n",
      "119 {'+': 103, 'N': 1543, 'V': 444, '~': 4} 2094 2094\n",
      "121 {'+': 1, 'A': 1, 'N': 1861, 'V': 1, '~': 12} 1876 1876\n",
      "122 {'+': 1, 'N': 2476, '|': 2} 2479 2479\n",
      "123 {'+': 1, 'N': 1515, 'V': 3} 1519 1519\n",
      "124 {'+': 13, 'A': 2, 'F': 5, 'J': 29, 'R': 1531, 'V': 47, 'j': 5, '~': 2} 1634 1634\n",
      "200 {'+': 148, 'A': 30, 'F': 2, 'N': 1743, 'V': 826, '~': 43} 2792 2792\n",
      "201 {'+': 35, 'A': 30, 'F': 2, 'J': 1, 'N': 1625, 'V': 198, 'a': 97, 'j': 10, 'x': 37, '~': 4} 2039 2039\n",
      "202 {'+': 8, 'A': 36, 'F': 1, 'N': 2061, 'V': 19, 'a': 19, '|': 2} 2146 2146\n",
      "203 {'+': 45, 'F': 1, 'N': 2529, 'Q': 4, 'V': 444, 'a': 2, '|': 26, '~': 57} 3108 3108\n",
      "205 {'+': 13, 'A': 3, 'F': 11, 'N': 2571, 'V': 71, '|': 1, '~': 2} 2672 2672\n",
      "207 {'!': 472, '+': 24, 'A': 107, 'E': 105, 'L': 1457, 'R': 86, 'V': 105, '[': 6, ']': 6, '|': 2, '~': 15} 2385 2385\n",
      "208 {'+': 53, 'F': 373, 'N': 1586, 'Q': 2, 'S': 2, 'V': 992, '|': 8, '~': 24} 3040 3040\n",
      "209 {'+': 21, 'A': 383, 'N': 2621, 'V': 1, '|': 7, '~': 19} 3052 3052\n",
      "210 {'+': 17, 'E': 1, 'F': 10, 'N': 2423, 'V': 194, 'a': 22, '|': 1, '~': 17} 2685 2685\n",
      "212 {'+': 1, 'N': 923, 'R': 1825, '|': 1, '~': 13} 2763 2763\n",
      "213 {'+': 43, 'A': 25, 'F': 362, 'N': 2641, 'V': 220, 'a': 3} 3294 3294\n",
      "214 {'\"': 1, '+': 25, 'F': 1, 'L': 2003, 'Q': 2, 'V': 256, '|': 5, '~': 4} 2297 2297\n",
      "215 {'\"': 2, '+': 5, 'A': 3, 'F': 1, 'N': 3195, 'V': 164, '~': 30} 3400 3400\n",
      "217 {'+': 67, '/': 1542, 'N': 244, 'V': 162, 'f': 260, '|': 1, '~': 4} 2280 2280\n",
      "219 {'\"': 4, '+': 21, 'A': 7, 'F': 1, 'N': 2082, 'V': 64, 'x': 133} 2312 2312\n",
      "220 {'+': 17, 'A': 94, 'N': 1954, '~': 4} 2069 2069\n",
      "221 {'+': 23, 'N': 2031, 'V': 396, '~': 12} 2462 2462\n",
      "222 {'+': 136, 'A': 208, 'J': 1, 'N': 2062, 'j': 212, '~': 15} 2634 2634\n",
      "223 {'+': 28, 'A': 72, 'F': 14, 'N': 2029, 'V': 473, 'a': 1, 'e': 16, '~': 10} 2643 2643\n",
      "228 {'\"': 3, '+': 41, 'A': 3, 'N': 1688, 'V': 362, '|': 24, '~': 20} 2141 2141\n",
      "230 {'+': 207, 'N': 2255, 'V': 1, '|': 1, '~': 2} 2466 2466\n",
      "231 {'\"': 427, '+': 11, 'A': 1, 'N': 314, 'R': 1254, 'V': 2, 'x': 2} 2011 2011\n",
      "232 {'+': 1, 'A': 1382, 'R': 397, 'j': 1, '~': 35} 1816 1816\n",
      "233 {'+': 71, 'A': 7, 'F': 11, 'N': 2230, 'V': 831, '|': 2} 3152 3152\n",
      "234 {'+': 3, 'J': 50, 'N': 2700, 'V': 3, '~': 8} 2764 2764\n"
     ]
    }
   ],
   "source": [
    "for file in annot_files:\n",
    "    path_file = os.path.join(file_path, file)\n",
    "    annotation = wfdb.rdann(path_file, 'atr')\n",
    "    unique, counts = np.unique(annotation.symbol, return_counts=True)\n",
    "    print(file, dict(zip(unique, counts)), counts.sum(), len(annotation.sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = []\n",
    "all_labels = []\n",
    "\n",
    "for i in range(48):\n",
    "    data, field = wfdb.rdsamp(os.path.join(file_path, data_files[i]))\n",
    "    data = data[:, 0]\n",
    "    \n",
    "    annot = wfdb.rdann(os.path.join(file_path, annot_files[i]), 'atr')\n",
    "    segmented_signals = [data[max(0, peak - 64):min(len(data), peak + 64)] for peak in annot.sample]\n",
    "    \n",
    "    segmented_array = np.array([\n",
    "        np.pad(signal, (0, 128 - len(signal)), mode='edge') if len(signal) < 128 else signal\n",
    "        for signal in segmented_signals\n",
    "    ])\n",
    "    \n",
    "    labels = annot.symbol[:len(segmented_array)]  \n",
    "\n",
    "    all_signals.append(segmented_array)\n",
    "    all_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_X_train, f_X_test, f_y_train, f_y_test = [], [], [], []\n",
    "for i in range(48):\n",
    "    X = np.array(all_signals[i]).reshape(-1, 128)\n",
    "    y = np.array(all_labels[i]).flatten()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.25,\n",
    "        random_state=42\n",
    "    )\n",
    "    f_X_train.append(X_train)\n",
    "    f_X_test.append(X_test)\n",
    "    f_y_train.extend(y_train)  \n",
    "    f_y_test.extend(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Shape: (84470, 128, 1), Test Shape: (28177, 128, 1)\n",
      "Final Train Labels Shape: (84470, 23), Test Labels Shape: (28177, 23)\n"
     ]
    }
   ],
   "source": [
    "f_X_train = np.vstack(f_X_train).reshape(-1, 128, 1)\n",
    "f_X_test = np.vstack(f_X_test).reshape(-1, 128, 1)\n",
    "y_train_int = [char_to_int.get(char, -1) for char in f_y_train]\n",
    "y_test_int = [char_to_int.get(char, -1) for char in f_y_test]\n",
    "y_train_int = [y for y in y_train_int if y != -1]\n",
    "y_test_int = [y for y in y_test_int if y != -1]\n",
    "num_classes = len(char_to_int)\n",
    "f_y_train = to_categorical(y_train_int, num_classes=num_classes)\n",
    "f_y_test = to_categorical(y_test_int, num_classes=num_classes)\n",
    "print(f\"Final Train Shape: {f_X_train.shape}, Test Shape: {f_X_test.shape}\")\n",
    "print(f\"Final Train Labels Shape: {f_y_train.shape}, Test Labels Shape: {f_y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGModel(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super(ECGModel, self).__init__()\n",
    "\n",
    "        feature_extractor = config[8]\n",
    "        sequence_model = config[9]\n",
    "        num_cnn_layers = int(config[0])\n",
    "        num_rnn_layers = int(config[1])\n",
    "        dropout = config[3]\n",
    "        num_filters = int(config[4])\n",
    "        kernel_size = int(config[5])\n",
    "        stride = int(config[6])\n",
    "        hidden_size = int(config[10])\n",
    "        fc_neurons = int(config[11])\n",
    "\n",
    "        self.input_size = 128  # Set initial input size\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "\n",
    "        for _ in range(num_cnn_layers):\n",
    "            print(f\"🔹 Before Conv Layer: {self.input_size}\")\n",
    "    \n",
    "            layers.append(nn.Conv1d(in_channels, num_filters, kernel_size=kernel_size, stride=stride, padding=1))\n",
    "            layers.append(nn.BatchNorm1d(num_filters))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_channels = num_filters\n",
    "\n",
    "            output_size = ((self.input_size - kernel_size + 2 * 1) // stride) + 1\n",
    "            print(f\"🔹 After Conv Layer: {output_size}\")\n",
    "\n",
    "            self.input_size = output_size\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        # Final feature output size\n",
    "        self.feature_output_size = self.input_size  \n",
    "\n",
    "        # RNN Configuration\n",
    "        if feature_extractor == 1:\n",
    "            self.rnn = nn.LSTM(input_size=self.feature_output_size, hidden_size=hidden_size, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = None\n",
    "\n",
    "        # Choose between LSTM or GRU\n",
    "        if sequence_model == 0:\n",
    "            self.rnn_layer = nn.LSTM(input_size=self.feature_output_size, hidden_size=hidden_size, num_layers=num_rnn_layers, batch_first=True, dropout=dropout) \n",
    "        else:\n",
    "            self.rnn_layer = nn.GRU(input_size=self.feature_output_size, hidden_size=hidden_size, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Linear(hidden_size, fc_neurons)\n",
    "        self.output = nn.Linear(fc_neurons, num_classes)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"🔍 Before CNN: {x.shape}\")  # Expect (batch, 1, seq_len)\n",
    "\n",
    "        x = self.feature_extractor(x)\n",
    "        print(f\"🔍 After CNN: {x.shape}\")  # Expect (batch, num_filters, seq_len)\n",
    "\n",
    "        # Ensure correct feature order for LSTM\n",
    "        x = x.permute(0, 2, 1) if self.feature_extractor else x  # Now should be (batch, seq_len, features)\n",
    "        print(f\"🔍 After Permute (before RNN): {x.shape}\")\n",
    "\n",
    "        assert x.shape[-1] == self.feature_output_size, f\"🚨 Mismatch: Expected {self.feature_output_size}, but got {x.shape[-1]}\"\n",
    "\n",
    "        x, _ = self.rnn_layer(x)  # Pass through LSTM\n",
    "        print(f\"🔍 After RNN: {x.shape}\")  # Expect (batch, seq_len, hidden_size)\n",
    "\n",
    "        x = self.fc(x[:, -1, :])  # Take last time step\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(config):\n",
    "    formatted_config = \"_\".join([f\"{x:.4f}\" if isinstance(x, float) else str(x) for x in config])\n",
    "    formatted_config = re.sub(r'[^\\w\\-_]', '', formatted_config)\n",
    "    return formatted_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serializable(obj):\n",
    "    \"\"\" Converts NumPy arrays and other non-serializable objects to lists for JSON serialization \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert NumPy array to a list\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]  # Recursively process lists\n",
    "    return obj  # Return normal values unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_time = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.shape[0]\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(inputs)\n",
    "            end_time = time.perf_counter()\n",
    "            total_time += (end_time - start_time) * 1000\n",
    "            num_samples += batch_size\n",
    "\n",
    "    return total_time / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_shape(tensor, expected_features):\n",
    "    print(f\"🔥 Input tensor shape: {tensor.shape}\")  # Debugging\n",
    "    if tensor.shape[-1] != expected_features:\n",
    "        raise ValueError(f\"Expected input features: {expected_features}, but got: {tensor.shape[-1]}\")\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Create a temporary model to get feature size\n",
    "    num_classes = f_y_train.shape[1]\n",
    "    temp_model = ECGModel(config, num_classes)\n",
    "\n",
    "    print(f\"✅ CNN Output Size: {temp_model.feature_output_size}\")\n",
    "    expected_features = temp_model.feature_output_size  # ✅ Fixed this line\n",
    "\n",
    "    # Check dataset shape before passing\n",
    "    print(f\"✅ Raw X_train shape: {f_X_train.shape}\")\n",
    "    \n",
    "    X_train, X_test = (\n",
    "        check_input_shape(torch.tensor(f_X_train, dtype=torch.float32).permute(0, 2, 1), expected_features),\n",
    "        check_input_shape(torch.tensor(f_X_test, dtype=torch.float32).permute(0, 2, 1), expected_features),\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Adjusted X_train shape: {X_train.shape}\")  # Should match `expected_features`\n",
    "    y_train, y_test = torch.tensor(f_y_train, dtype=torch.long), torch.tensor(f_y_test, dtype=torch.long)\n",
    "\n",
    "    batch_size = int(config[7]) if len(config) > 7 else 64\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Now create the actual model\n",
    "    model = ECGModel(config, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[2])\n",
    "\n",
    "    num_epochs = 10\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            true_labels = labels.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(true_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    latency = measure_latency(model, test_loader, device)\n",
    "    fitness = accuracy - (latency / 100)\n",
    "\n",
    "    model_name = f\"model_{format_filename(config)}.pt\"\n",
    "    torch.save(model.state_dict(), f\"models/{model_name}\")\n",
    "\n",
    "    serializable_config = convert_to_serializable(config)\n",
    "    result = {\"config\": serializable_config, \"accuracy\": accuracy, \"latency\": latency, \"fitness\": fitness}\n",
    "    with open(f\"results/{model_name}.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "\n",
    "    print(f\"✅ Config: {serializable_config} -> Accuracy: {accuracy:.4f}, Latency: {latency:.2f}ms, Fitness: {fitness:.4f} | Model Saved: {model_name}\")\n",
    "    return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Before Conv Layer: 128\n",
      "🔹 After Conv Layer: 64\n",
      "🔹 Before Conv Layer: 64\n",
      "🔹 After Conv Layer: 32\n",
      "✅ CNN Output Size: 32\n",
      "✅ Raw X_train shape: (84470, 128, 1)\n",
      "🔥 Input tensor shape: torch.Size([84470, 1, 128])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input features: 32, but got: 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m problem_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: bounds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_func\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_and_evaluate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     18\u001b[0m ga_model \u001b[38;5;241m=\u001b[39m BaseGA(epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, pop_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, pc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, pm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m best_solution \u001b[38;5;241m=\u001b[39m \u001b[43mga_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m best_architecture \u001b[38;5;241m=\u001b[39m best_solution\u001b[38;5;241m.\u001b[39msolution\n\u001b[0;32m     22\u001b[0m best_fitness \u001b[38;5;241m=\u001b[39m best_solution\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mfitness\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mealpy\\optimizer.py:223\u001b[0m, in \u001b[0;36mOptimizer.solve\u001b[1;34m(self, problem, mode, n_workers, termination, starting_solutions, seed)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem: Union[Dict, Problem] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m'\u001b[39m, n_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    203\u001b[0m           termination: Union[Dict, Termination] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, starting_solutions: Union[List, np\u001b[38;5;241m.\u001b[39mndarray, Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    204\u001b[0m           seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Agent:\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        problem: an instance of Problem class or a dictionary\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m        g_best: g_best, the best found agent, that hold the best solution and the best target. Access by: .g_best.solution, .g_best.target\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_mode_and_workers(mode, n_workers)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_termination(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, termination, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mealpy\\optimizer.py:157\u001b[0m, in \u001b[0;36mOptimizer.check_problem\u001b[1;34m(self, problem, seed)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(problem) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    156\u001b[0m     problem[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m=\u001b[39m \u001b[43mProblem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproblem needs to be a dict or an instance of Problem class.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mealpy\\utils\\problem.py:28\u001b[0m, in \u001b[0;36mProblem.__init__\u001b[1;34m(self, bounds, minmax, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_keyword_arguments(kwargs)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(bounds)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__set_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m Logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_to, log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_file)\u001b[38;5;241m.\u001b[39mcreate_logger(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m                             format_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m [line: \u001b[39m\u001b[38;5;132;01m%(lineno)d\u001b[39;00m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mealpy\\utils\\problem.py:65\u001b[0m, in \u001b[0;36mProblem.__set_functions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m tested_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_solution(encoded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tested_solution)\n\u001b[1;32m---> 65\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtested_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(result) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSUPPORTED_ARRAYS:\n\u001b[0;32m     67\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[1;32mIn[205], line 15\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check dataset shape before passing\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Raw X_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_X_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mcheck_input_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_features\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     16\u001b[0m     check_input_shape(torch\u001b[38;5;241m.\u001b[39mtensor(f_X_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), expected_features),\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Adjusted X_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should match `expected_features`\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(f_y_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong), torch\u001b[38;5;241m.\u001b[39mtensor(f_y_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[198], line 4\u001b[0m, in \u001b[0;36mcheck_input_shape\u001b[1;34m(tensor, expected_features)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔥 Input tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m expected_features:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input features: 32, but got: 128"
     ]
    }
   ],
   "source": [
    "bounds = [\n",
    "    IntegerVar(lb=1, ub=5),\n",
    "    IntegerVar(lb=1, ub=5),\n",
    "    FloatVar(lb=0.0001, ub=0.01),\n",
    "    FloatVar(lb=0.1, ub=0.5),\n",
    "    IntegerVar(lb=32, ub=128),\n",
    "    IntegerVar(lb=3, ub=7),\n",
    "    IntegerVar(lb=1, ub=2),\n",
    "    IntegerVar(lb=16, ub=128),\n",
    "    IntegerVar(lb=0, ub=1),\n",
    "    IntegerVar(lb=0, ub=1),\n",
    "    IntegerVar(lb=64, ub=512),\n",
    "    IntegerVar(lb=32, ub=256),\n",
    "]\n",
    "\n",
    "problem_dict = {\"bounds\": bounds, \"obj_func\": train_and_evaluate, \"minmax\": \"max\"}\n",
    "\n",
    "ga_model = BaseGA(epoch=10, pop_size=5, pc=0.9, pm=0.05)\n",
    "best_solution = ga_model.solve(problem_dict)\n",
    "\n",
    "best_architecture = best_solution.solution\n",
    "best_fitness = best_solution.target.fitness\n",
    "\n",
    "best_model_info = {\"best_architecture\": best_architecture, \"best_fitness\": best_fitness}\n",
    "with open(\"results/best_model.json\", \"w\") as f:\n",
    "    json.dump(best_model_info, f, indent=4)\n",
    "\n",
    "print(f\"\\n🏆 Best Model Configuration: {best_architecture}, Fitness Score: {best_fitness}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
